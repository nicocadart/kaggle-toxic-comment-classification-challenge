{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutoriel for pre-processing in the Toxic Comment challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA\n"
     ]
    }
   ],
   "source": [
    "data_train, y_train, data_test, id_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMENT BEFORE CLEANING: \n",
      " Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "-------\n",
      " Removing ip adresses, \\n, capitals, accents, stopwords ...\n",
      "COMMENT AFTER CLEANING: \n",
      " explanation edit make username hardcore metallica fan revert vandalisms closure gas vote new york dolls fac please remove template talk page since i m retire \n"
     ]
    }
   ],
   "source": [
    "comment = data_train[0]\n",
    "print('COMMENT BEFORE CLEANING: \\n {}'.format(comment))\n",
    "print('-------\\n Removing ip adresses, \\\\n, capitals, accents, stopwords ...')\n",
    "print('COMMENT AFTER CLEANING: \\n {}'.format(clean_comment(comment)))\n",
    "\n",
    "# Stopswords trop large ? dont a disparu, et avec lui le sens n√©gatif de la phrase ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation: 0.0%\r",
      "Transformation: 10.0%\r",
      "Transformation: 20.0%\r",
      "Transformation: 30.0%\r",
      "Transformation: 40.0%\r",
      "Transformation: 50.0%\r",
      "Transformation: 60.0%\r",
      "Transformation: 70.0%\r",
      "Transformation: 80.0%\r",
      "Transformation: 90.0%\r"
     ]
    }
   ],
   "source": [
    "kwargs = {'maxlen':100, 'join_bool':False}\n",
    "transform_dtset = transform_dataset(data_train[:10], func=pad_comment, kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 26, 46, 126, 17, 15, 8, 23, 99, 12]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(st) for st in transform_dtset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------\n",
    "### CBOW\n",
    "\n",
    "# Create a CBOW vectorizer for english words, without accent,\n",
    "## limiting the vocabulary to 30000 words max.\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer='word', stop_words='english',\n",
    "                                   strip_accents='unicode', max_features=30000)\n",
    "\n",
    "# -------\n",
    "### Hash\n",
    "\n",
    "# Create a CBOW vectorizer for english words, without accent. No limit on vocab size\n",
    "\n",
    "hash_vectorizer = HashingVectorizer(analyzer='word', stop_words='english',\n",
    "                                     strip_accents='unicode')\n",
    "\n",
    "\n",
    "# --------\n",
    "### TFIDF\n",
    "\n",
    "# Create a TFIDF vectorizer for english words, (only unigrams), limiting the vocabulary to\n",
    "# 30000 words max.and filtering words with frequency under 10.\n",
    "## Remove accents, and using idf for filtering, with smoothing to avoid zero division\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', analyzer='word', ngram_range=(1,1),\n",
    "                                   min_df=10, max_features=30000,\n",
    "                                   strip_accents='unicode', use_idf=1,smooth_idf=1,\n",
    "                                   sublinear_tf=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = encode(data_train, data_test, vectorizer=count_vectorizer)\n",
    "\n",
    "# Vocabulary can be extracted from the vectorizer object (if tdidf or count)\n",
    "# print(count_vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting data to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy prediction to test submission\n",
    "y_test = np.ones((len(data_test), y_train.shape[1]))\n",
    "\n",
    "\n",
    "submission(y_train, id_test, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
