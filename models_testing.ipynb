{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-traitement des données\n",
    "\n",
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GlobalMaxPooling1D, Bidirectional, Conv1D, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "from tools import *\n",
    "from embeddings import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw string data\n",
    "data_train, y_train_all, data_test, id_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des données textuelles (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
      "-------\n",
      "Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n"
     ]
    }
   ],
   "source": [
    "params = {'clean': False,\n",
    "          'lower': False, \n",
    "          'lemma': False, \n",
    "          'stop_words': False}\n",
    "\n",
    "comment = data_train[2]\n",
    "print(comment)\n",
    "print('-------')\n",
    "print(CommentCleaner(**params).transform(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation: 100%       \n",
      "Transformation: 100%       \n"
     ]
    }
   ],
   "source": [
    "clean_data_train = transform_dataset(data_train, transformer=CommentCleaner, kwargs=params)\n",
    "clean_data_test = transform_dataset(data_test, transformer=CommentCleaner, kwargs=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion numérique des données textuelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODING: Fitting vectorizer to data\n",
      "ENCODING: transforming data to numerical\n"
     ]
    }
   ],
   "source": [
    "# Convert strings to int indexes, \n",
    "# considering only the VOCAB_SIZE most common words, \n",
    "# and pad the sentences to SENTENCE_LENGTH words\n",
    "VOCAB_SIZE = 30000\n",
    "SENTENCE_LENGTH = 200  # 200 if stop_words deleted, 120 otherwise\n",
    "\n",
    "tokenizer = TokenVectorizer(max_len=SENTENCE_LENGTH, max_features=VOCAB_SIZE)\n",
    "\n",
    "# X_train_all, X_test = encode(data_train, data_test, vectorizer=tokens_vectorizer)\n",
    "X_train_all, X_test = encode(clean_data_train, clean_data_test, vectorizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction des features auxiliaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing comments length\n",
      "Transformation: 100%       \n",
      "Transformation: 100%       \n",
      "Computing number of punctuation marks in comments\n",
      "Transformation: 100%       \n",
      "Transformation: 100%       \n",
      "Computing number of upper cased words in comments\n",
      "Transformation: 100%       \n",
      "Transformation: 100%       \n"
     ]
    }
   ],
   "source": [
    "print(\"Computing comments length\")\n",
    "comments_lengths_train = np.array(transform_dataset(data_train, transformer=CommentLength, n_prints=5))\n",
    "comments_lengths_test = np.array(transform_dataset(data_test, transformer=CommentLength, n_prints=5))\n",
    "\n",
    "print(\"Computing number of punctuation marks in comments\")\n",
    "params = {'divide_by_len': True, 'chars_set': {'!'}}\n",
    "comments_punctuation_train = np.array(transform_dataset(data_train, transformer=CharCounter, kwargs=params))\n",
    "comments_punctuation_test = np.array(transform_dataset(data_test, transformer=CharCounter, kwargs=params))\n",
    "\n",
    "print(\"Computing number of upper cased words in comments\")\n",
    "params = {'divide_by_len': True}\n",
    "comments_upperwords_train = np.array(transform_dataset(data_train, transformer=UppercaseWordsCounter, kwargs=params))\n",
    "comments_upperwords_test = np.array(transform_dataset(data_test, transformer=UppercaseWordsCounter, kwargs=params))\n",
    "\n",
    "# concatenation of auxiliary features\n",
    "X_aux_train_all = np.vstack((comments_lengths_train, comments_punctuation_train, comments_upperwords_train)).T\n",
    "X_aux_test = np.vstack((comments_lengths_test, comments_punctuation_test, comments_upperwords_test)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation du jeu d'entraînement et de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_VALID_RATIO = 0.10\n",
    "SPLIT_RANDOM_SEED = 233  # TODO : check split because of imbalanced classes\n",
    "\n",
    "# numerical comments\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all, \n",
    "                                                      test_size=SPLIT_VALID_RATIO,\n",
    "                                                      random_state=SPLIT_RANDOM_SEED)\n",
    "\n",
    "# auxiliary input\n",
    "X_aux_train, X_aux_valid, _, _ = train_test_split(X_aux_train_all, y_train_all, \n",
    "                                                      test_size=SPLIT_VALID_RATIO,\n",
    "                                                      random_state=SPLIT_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test des différents modèles\n",
    "\n",
    "## Embeddings + LSTM + 2 fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 150\n",
    "TRAIN_EMBEDDINGS = True\n",
    "MODEL_NAME = \"draft_embed_bidirlstm_2fc\"\n",
    "\n",
    "model = bidirectional_lstm(sentence_length=SENTENCE_LENGTH, vocab_size=VOCAB_SIZE,\n",
    "                    embedding_dim=EMBEDDING_DIM, embedding_matrix=None, train_embeddings=TRAIN_EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1380s 10ms/step - loss: 0.0633 - acc: 0.9789 - val_loss: 0.0469 - val_acc: 0.9826\n",
      "epoch: 1 - val_roc_auc: 0.9765\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1445s 10ms/step - loss: 0.0432 - acc: 0.9836 - val_loss: 0.0456 - val_acc: 0.9832\n",
      "epoch: 2 - val_roc_auc: 0.9815\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 2\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=([X_valid, X_aux_valid] if USE_AUX_FEATURES else X_valid, y_valid))\n",
    "\n",
    "hist = model.fit([X_train, X_aux_train] if USE_AUX_FEATURES else X_train, y_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=N_EPOCHS, \n",
    "                 validation_data=([X_valid, X_aux_valid] if USE_AUX_FEATURES else X_valid, y_valid),\n",
    "                 callbacks=[RocAuc])\n",
    "\n",
    "# save trained nnet to disk for later use\n",
    "save_nnet(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model evaluation\n",
    "y_train_pred = model.predict([X_train, X_aux_train] if USE_AUX_FEATURES else X_train, batch_size=512)\n",
    "train_score = evaluate(y_train, y_train_pred)\n",
    "print(\"ROC-AUC score on train set : {:.4f}\".format(train_score))\n",
    "\n",
    "y_valid_pred = model.predict([X_valid, X_aux_valid] if USE_AUX_FEATURES else X_valid, batch_size=512)\n",
    "valid_score = evaluate(y_valid, y_valid_pred)\n",
    "print(\"ROC-AUC score on validation set : {:.4f}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings + conv1D parallèles + fc (Yoon Kim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe pre-trained embeddings\n",
    "EMBEDDING_DIM = 300  # several embeddings sizes depending on source : 25, 50, 100, 200, 300 \n",
    "EMBEDDING_SOURCE = 'fasttext_crawl'  # {'glove_twitter', 'glove_wikipedia', 'word2vec_googlenews', 'fasttext_crawl'}\n",
    "\n",
    "embeddings_matrix = load_pretrained_embeddings(tokenizer.word_index, VOCAB_SIZE, EMBEDDING_DIM, EMBEDDING_SOURCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_AUX_FEATURES = False\n",
    "TRAIN_EMBEDDINGS = True\n",
    "N_FILTERS = 100\n",
    "FILTERS_SIZES = (3, 5, 7)\n",
    "MODEL_NAME = \"embed_conv_fc\"\n",
    "\n",
    "model = yoon_kim(sentence_length=SENTENCE_LENGTH, vocab_size=VOCAB_SIZE,\n",
    "                 n_filters=N_FILTERS, filters_sizes=FILTERS_SIZES,\n",
    "                 embedding_dim=EMBEDDING_DIM, embedding_matrix=None, train_embeddings=TRAIN_EMBEDDINGS,\n",
    "                 aux_input_dim=X_aux_train.shape[1] if USE_AUX_FEATURES else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 2\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=([X_valid, X_aux_valid] if USE_AUX_FEATURES else X_valid, y_valid))\n",
    "\n",
    "hist = model.fit([X_train, X_aux_train] if USE_AUX_FEATURES else X_train, y_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=N_EPOCHS, \n",
    "                 validation_data=([X_valid, X_aux_valid] if USE_AUX_FEATURES else X_valid, y_valid),\n",
    "                 callbacks=[RocAuc])\n",
    "\n",
    "# save trained nnet to disk for later use\n",
    "save_nnet(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model evaluation\n",
    "y_train_pred = model.predict([X_train, X_aux_train] if USE_AUX_FEATURES else X_train, batch_size=512)\n",
    "train_score = evaluate(y_train, y_train_pred)\n",
    "print(\"ROC-AUC score on train set : {:.4f}\".format(train_score))\n",
    "\n",
    "y_valid_pred = model.predict([X_valid, X_aux_valid] if USE_AUX_FEATURES else X_valid, batch_size=512)\n",
    "valid_score = evaluate(y_valid, y_valid_pred)\n",
    "print(\"ROC-AUC score on validation set : {:.4f}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidir LSTM (+ auxiliary input?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pre-trained word vectors in database       : 1193514\n",
      "Number of our words with a pre-trained embedding     : 26798\n",
      "Percentage of our words with a pre-trained embedding : 89.327%\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe pre-trained embeddings\n",
    "EMBEDDING_DIM = 100  # several embeddings sizes depending on source : 25, 50, 100, 200, 300 \n",
    "EMBEDDING_SOURCE = 'glove_twitter'  # {'glove_twitter', 'glove_wikipedia', 'word2vec_googlenews', 'fasttext_crawl'}\n",
    "\n",
    "embeddings_matrix = load_pretrained_embeddings(tokenizer.word_index, VOCAB_SIZE, EMBEDDING_DIM, EMBEDDING_SOURCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_AUX_FEATURES = False\n",
    "TRAIN_EMBEDDINGS = True\n",
    "MODEL_NAME = \"bidirlstm_dropout_bipool_1fc_glove_twitter_100t\"\n",
    "\n",
    "model = bidirectional_lstm(sentence_length=SENTENCE_LENGTH, vocab_size=VOCAB_SIZE,\n",
    "                           embedding_dim=EMBEDDING_DIM, embedding_matrix=embeddings_matrix, train_embeddings=TRAIN_EMBEDDINGS,\n",
    "                           aux_input_dim=X_aux_train.shape[1] if USE_AUX_FEATURES else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1586s 11ms/step - loss: 0.0545 - acc: 0.9808 - val_loss: 0.0447 - val_acc: 0.9835\n",
      "epoch: 1 - val_roc_auc: 0.9834\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1709s 12ms/step - loss: 0.0400 - acc: 0.9844 - val_loss: 0.0431 - val_acc: 0.9836\n",
      "epoch: 2 - val_roc_auc: 0.9870\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 2\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=([X_valid, X_aux_valid] if USE_AUX_FEATURES else X_valid, y_valid))\n",
    "\n",
    "hist = model.fit([X_train, X_aux_train] if USE_AUX_FEATURES else X_train, y_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=N_EPOCHS, \n",
    "                 validation_data=([X_valid, X_aux_valid] if USE_AUX_FEATURES else X_valid, y_valid),\n",
    "                 callbacks=[RocAuc])\n",
    "\n",
    "# save trained nnet to disk for later use\n",
    "save_nnet(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score on train set : 0.9922\n",
      "ROC-AUC score on validation set : 0.9870\n"
     ]
    }
   ],
   "source": [
    "# final model evaluation\n",
    "y_train_pred = model.predict([X_train, X_aux_train] if USE_AUX_FEATURES else X_train, batch_size=512)\n",
    "train_score = evaluate(y_train, y_train_pred)\n",
    "print(\"ROC-AUC score on train set : {:.4f}\".format(train_score))\n",
    "\n",
    "y_valid_pred = model.predict([X_valid, X_aux_valid] if USE_AUX_FEATURES else X_valid, batch_size=512)\n",
    "valid_score = evaluate(y_valid, y_valid_pred)\n",
    "print(\"ROC-AUC score on validation set : {:.4f}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions et soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_test_pred = model.predict([X_test, X_aux_test] if USE_AUX_FEATURES else X_test, batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write submission file\n",
    "submission(y_test_pred, id_test, name=MODEL_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
