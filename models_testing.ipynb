{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-traitement des données\n",
    "\n",
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GlobalMaxPooling1D, Bidirectional, Conv1D, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "from tools import *\n",
    "from embeddings import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw string data\n",
    "data_train, y_train_all, data_test, id_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des données textuelles (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
      "-------\n",
      "hey man i m really not trying to edit war it s just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info \n"
     ]
    }
   ],
   "source": [
    "params = {'lower': True, \n",
    "          'lemma': False, \n",
    "          'stop_words': False}\n",
    "\n",
    "comment = data_train[2]\n",
    "print(comment)\n",
    "print('-------')\n",
    "print(CommentCleaner(**params).transform(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation: 100%       \n",
      "Transformation: 100%       \n"
     ]
    }
   ],
   "source": [
    "clean_data_train = transform_dataset(data_train, transformer=CommentCleaner, kwargs=params)\n",
    "clean_data_test = transform_dataset(data_test, transformer=CommentCleaner, kwargs=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion numérique des données textuelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODING: Fitting vectorizer to data\n",
      "ENCODING: transforming data to numerical\n"
     ]
    }
   ],
   "source": [
    "# Convert strings to int indexes, \n",
    "# considering only the VOCAB_SIZE most common words, \n",
    "# and pad the sentences to SENTENCE_LENGTH words\n",
    "VOCAB_SIZE = 30000\n",
    "SENTENCE_LENGTH = 200  # 200 if stop_words deleted, 120 otherwise\n",
    "\n",
    "tokenizer = TokenVectorizer(max_len=SENTENCE_LENGTH, max_features=VOCAB_SIZE)\n",
    "\n",
    "# X_train_all, X_test = encode(data_train, data_test, vectorizer=tokens_vectorizer)\n",
    "X_train_all, X_test = encode(clean_data_train, clean_data_test, vectorizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction des features auxiliaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing comments length\n",
      "Transformation: 100%       \n",
      "Transformation: 100%       \n",
      "Computing number of punctuation marks in comments\n",
      "Transformation: 100%       \n",
      "Transformation: 100%       \n",
      "Computing number of upper cased words in comments\n",
      "Transformation: 100%       \n",
      "Transformation: 100%       \n"
     ]
    }
   ],
   "source": [
    "print(\"Computing comments length\")\n",
    "comments_lengths_train = np.array(transform_dataset(data_train, transformer=CommentLength, n_prints=5))\n",
    "comments_lengths_test = np.array(transform_dataset(data_test, transformer=CommentLength, n_prints=5))\n",
    "\n",
    "print(\"Computing number of punctuation marks in comments\")\n",
    "params = {'divide_by_len': True, 'chars_set': {'!'}}\n",
    "comments_punctuation_train = np.array(transform_dataset(data_train, transformer=CharCounter, kwargs=params))\n",
    "comments_punctuation_test = np.array(transform_dataset(data_test, transformer=CharCounter, kwargs=params))\n",
    "\n",
    "print(\"Computing number of upper cased words in comments\")\n",
    "params = {'divide_by_len': True}\n",
    "comments_upperwords_train = np.array(transform_dataset(data_train, transformer=UppercaseWordsCounter, kwargs=params))\n",
    "comments_upperwords_test = np.array(transform_dataset(data_test, transformer=UppercaseWordsCounter, kwargs=params))\n",
    "\n",
    "# concatenation of auxiliary features\n",
    "X_aux_train_all = np.vstack((comments_lengths_train, comments_punctuation_train, comments_upperwords_train)).T\n",
    "X_aux_test = np.vstack((comments_lengths_test, comments_punctuation_test, comments_upperwords_test)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation du jeu d'entraînement et de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_VALID_RATIO = 0.10\n",
    "SPLIT_RANDOM_SEED = 0  # TODO : check split because of imbalanced classes\n",
    "\n",
    "# numerical comments\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all, \n",
    "                                                      test_size=SPLIT_VALID_RATIO,\n",
    "                                                      random_state=SPLIT_RANDOM_SEED)\n",
    "\n",
    "# auxiliary input\n",
    "X_aux_train, X_aux_valid, _, _ = train_test_split(X_aux_train_all, y_train_all, \n",
    "                                                      test_size=SPLIT_VALID_RATIO,\n",
    "                                                      random_state=SPLIT_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test des différents modèles\n",
    "\n",
    "## Embeddings + LSTM + 2 fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 150\n",
    "TRAIN_EMBEDDINGS = True\n",
    "MODEL_NAME = \"draft_embed_bidirlstm_2fc\"\n",
    "\n",
    "model = bidirectional_lstm(sentence_length=SENTENCE_LENGTH, vocab_size=VOCAB_SIZE,\n",
    "                    embedding_dim=EMBEDDING_DIM, embedding_matrix=None, train_embeddings=TRAIN_EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1380s 10ms/step - loss: 0.0633 - acc: 0.9789 - val_loss: 0.0469 - val_acc: 0.9826\n",
      "epoch: 1 - val_roc_auc: 0.9765\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1445s 10ms/step - loss: 0.0432 - acc: 0.9836 - val_loss: 0.0456 - val_acc: 0.9832\n",
      "epoch: 2 - val_roc_auc: 0.9815\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 2\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_valid, y_valid))\n",
    "\n",
    "hist = model.fit(X_train, y_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=N_EPOCHS, \n",
    "                 validation_data=(X_valid, y_valid),\n",
    "                 callbacks=[RocAuc])\n",
    "\n",
    "# save trained nnet to disk for later use\n",
    "save_nnet(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score on train set : 0.9898\n",
      "ROC-AUC score on validation set : 0.9815\n"
     ]
    }
   ],
   "source": [
    "# final model evaluation\n",
    "y_train_pred = model.predict(X_train, batch_size=512)\n",
    "train_score = evaluate(y_train, y_train_pred)\n",
    "print(\"ROC-AUC score on train set : {:.4f}\".format(train_score)) \n",
    "\n",
    "y_valid_pred = model.predict(X_valid, batch_size=512)\n",
    "valid_score = evaluate(y_valid, y_valid_pred)\n",
    "print(\"ROC-AUC score on validation set : {:.4f}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_test_pred = model.predict(X_test, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write submission file\n",
    "submission(y_test_pred, id_test, name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings + conv1D parallèles + fc (Yoon Kim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 150\n",
    "TRAIN_EMBEDDINGS = True\n",
    "N_FILTERS = 100\n",
    "FILTERS_SIZES = (3, 5, 7)\n",
    "MODEL_NAME = \"embed_conv_fc\"\n",
    "\n",
    "model = yoon_kim(sentence_length=SENTENCE_LENGTH, vocab_size=VOCAB_SIZE,\n",
    "                 n_filters=N_FILTERS, filters_sizes=FILTERS_SIZES,\n",
    "                 embedding_dim=EMBEDDING_DIM, embedding_matrix=None, train_embeddings=TRAIN_EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1145s 8ms/step - loss: 0.0598 - acc: 0.9793 - val_loss: 0.0466 - val_acc: 0.9827\n",
      "epoch: 1 - val_roc_auc: 0.9800\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1081s 8ms/step - loss: 0.0418 - acc: 0.9838 - val_loss: 0.0441 - val_acc: 0.9836\n",
      "epoch: 2 - val_roc_auc: 0.9846\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/embed_conv_fc.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cda0c6d3c0e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# save trained nnet to disk for later use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0msave_nnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Académique/M2_AIC/OPT7_DeepStruct/kaggle-toxic-comment-classification-challenge/tools.py\u001b[0m in \u001b[0;36msave_nnet\u001b[0;34m(model, name, dir)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;31m# serialize model to JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mmodel_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}{}.json\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;31m# serialize weights to HDF5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/embed_conv_fc.json'"
     ]
    }
   ],
   "source": [
    "# train\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 2\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_valid, y_valid))\n",
    "\n",
    "hist = model.fit(X_train, y_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=N_EPOCHS, \n",
    "                 validation_data=(X_valid, y_valid),\n",
    "                 callbacks=[RocAuc])\n",
    "\n",
    "# save trained nnet to disk for later use\n",
    "save_nnet(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model evaluation\n",
    "y_train_pred = model.predict(X_train, batch_size=512)\n",
    "train_score = evaluate(y_train, y_train_pred)\n",
    "print(\"ROC-AUC score on train set : {:.4f}\".format(train_score)) \n",
    "\n",
    "y_valid_pred = model.predict(X_valid, batch_size=512)\n",
    "valid_score = evaluate(y_valid, y_valid_pred)\n",
    "print(\"ROC-AUC score on validation set : {:.4f}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe twitter 200 + Bidir LSTM + auxiliary input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pre-trained word vectors in database       : 1193514\n",
      "Number of our words with a pre-trained embedding     : 26798\n",
      "Percentage of our words with a pre-trained embedding : 89.327%\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe pre-trained embeddings\n",
    "EMBEDDING_DIM = 200  # several embeddings sizes depending on source : 25, 50, 100, 200, 300 \n",
    "EMBEDDING_SOURCE = 'glove_twitter'  # {'glove_twitter', 'glove_wikipedia', 'word2vec_googlenews'}\n",
    "\n",
    "embeddings_matrix = load_pretrained_embeddings(tokenizer.word_index, VOCAB_SIZE, EMBEDDING_DIM, EMBEDDING_SOURCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_AUX_FEATURES = True\n",
    "TRAIN_EMBEDDINGS = True\n",
    "MODEL_NAME = \"bidirlstm_bipool_aux_1fc_glove_twitter_200t\"\n",
    "\n",
    "model = bidirectional_lstm(sentence_length=SENTENCE_LENGTH, vocab_size=VOCAB_SIZE,\n",
    "                           embedding_dim=EMBEDDING_DIM, embedding_matrix=embeddings_matrix, train_embeddings=TRAIN_EMBEDDINGS,\n",
    "                           aux_input_dim=X_aux_train.shape[1] if USE_AUX_FEATURES else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1945s 14ms/step - loss: 0.0712 - acc: 0.9788 - val_loss: 0.0429 - val_acc: 0.9839\n",
      "epoch: 1 - val_roc_auc: 0.9848\n",
      "Epoch 2/2\n",
      " 41920/143613 [=======>......................] - ETA: 21:31 - loss: 0.0385 - acc: 0.9848"
     ]
    }
   ],
   "source": [
    "# train\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 2\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=([X_valid, X_aux_valid] if USE_AUX_FEATURES else X_valid, y_valid))\n",
    "\n",
    "hist = model.fit([X_train, X_aux_train] if USE_AUX_FEATURES else X_train, y_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=N_EPOCHS, \n",
    "                 validation_data=([X_valid, X_aux_valid] if USE_AUX_FEATURES else X_valid, y_valid),\n",
    "                 callbacks=[RocAuc])\n",
    "\n",
    "# save trained nnet to disk for later use\n",
    "save_nnet(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model evaluation\n",
    "y_train_pred = model.predict([X_train, X_aux_train] if USE_AUX_FEATURES else X_train, batch_size=512)\n",
    "train_score = evaluate(y_train, y_train_pred)\n",
    "print(\"ROC-AUC score on train set : {:.4f}\".format(train_score))\n",
    "\n",
    "y_valid_pred = model.predict([X_valid, X_aux_valid] if USE_AUX_FEATURES else X_valid, batch_size=512)\n",
    "valid_score = evaluate(y_valid, y_valid_pred)\n",
    "print(\"ROC-AUC score on validation set : {:.4f}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions et soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_test_pred = model.predict([X_test, X_aux_test] if USE_AUX_FEATURES else X_test, batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write submission file\n",
    "submission(y_test_pred, id_test, name=MODEL_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
