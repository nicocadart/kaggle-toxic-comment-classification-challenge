{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan de ce notebook\n",
    "\n",
    "1. Use pre-trained GloVe words for embeddings\n",
    "2. Use pre-trained word2vec words for embeddings\n",
    "3. What about Freebase ?\n",
    "4. Use pre-trained GloVe words for embeddings with LSTM model\n",
    "\n",
    "# Nettoyage et conversion numérique des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from tools import *\n",
    "from embeddings import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw string data\n",
    "data_train, y_train_all, data_test, id_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des données (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
      "-------\n",
      "hey man i m really not trying to edit war it s just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info \n"
     ]
    }
   ],
   "source": [
    "params = {'lower': True, \n",
    "          'lemma': False, \n",
    "          'stop_words': False}\n",
    "\n",
    "comment = data_train[2]\n",
    "print(comment)\n",
    "print('-------')\n",
    "print(CommentCleaner(**params).transform(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation: 100%       \n",
      "Transformation: 100%       \n"
     ]
    }
   ],
   "source": [
    "clean_data_train = transform_dataset(data_train, transformer=CommentCleaner, kwargs=params)\n",
    "clean_data_test = transform_dataset(data_test, transformer=CommentCleaner, kwargs=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization et découpage des données textuelles \n",
    "\n",
    "Conforme au github https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert strings to int indexes, \n",
    "# considering only the VOCAB_SIZE most commons words, \n",
    "# and pad the sentences to SENTENCE_LENGTH words\n",
    "VOCAB_SIZE = 30000\n",
    "SENTENCE_LENGTH = 200  # 200 if stop_words deleted, 120 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODING: Fitting vectorizer to data\n",
      "ENCODING: transforming data to numerical\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TokenVectorizer(max_len=SENTENCE_LENGTH, max_features=VOCAB_SIZE)\n",
    "\n",
    "# X_train_all, X_test = encode(data_train, data_test, vectorizer=tokens_vectorizer)\n",
    "X_train_all, X_test = encode(clean_data_train, clean_data_test, vectorizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_VALID_RATIO = 0.10\n",
    "SPLIT_RANDOM_SEED = 0  # because of unbalanced classes : check split --> done and OK\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all, \n",
    "                                                      test_size=SPLIT_VALID_RATIO,\n",
    "                                                      random_state=SPLIT_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Use pre-trained GloVe words for embeddings\n",
    "\n",
    "https://medium.com/@sabber/classifying-yelp-review-comments-using-cnn-lstm-and-pre-trained-glove-word-embeddings-part-3-53fcea9a17fa\n",
    "\n",
    "https://github.com/msahamed/yelp_comments_classification_nlp\n",
    "\n",
    "https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb\n",
    "\n",
    "## embedding_matrix avec les embeddings de Glove\n",
    "\n",
    "several possibilities of pre-training/embeddings vector sizes for GloVe, see: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pre-trained word vectors in database       : 400000\n",
      "Number of our words with a pre-trained embedding     : 27289\n",
      "Percentage of our words with a pre-trained embedding : 90.963%\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe pre-trained embeddings\n",
    "EMBEDDING_DIM = 200  # several embeddings sizes depending on source : 25, 50, 100, 200, 300 \n",
    "EMBEDDING_SOURCE = 'glove_wikipedia'  # {'glove_twitter', 'glove_wikipedia', 'word2vec_googlenews'}\n",
    "\n",
    "embeddings_matrix = load_pretrained_embeddings(tokenizer.word_index, VOCAB_SIZE, EMBEDDING_DIM, EMBEDDING_SOURCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du réseau de Yoon Kim pour GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FILTERS = 100\n",
    "FILTERS_SIZES = (3, 5, 7)\n",
    "TRAIN_EMBEDDINGS = True\n",
    "MODEL_NAME = \"embed_conv_fc_GLOVE_emb200_pretrained_trainableTrue_goodStatInit\"\n",
    "\n",
    "model = yoon_kim(sentence_length=SENTENCE_LENGTH, vocab_size=VOCAB_SIZE,\n",
    "                 n_filters=N_FILTERS, filters_sizes=FILTERS_SIZES,\n",
    "                 embedding_dim=EMBEDDING_DIM, embedding_matrix=embeddings_matrix, train_embeddings=TRAIN_EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1312s 9ms/step - loss: 0.0553 - acc: 0.9800 - val_loss: 0.0451 - val_acc: 0.9826\n",
      "epoch: 1 - val_roc_auc: 0.9849\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1253s 9ms/step - loss: 0.0413 - acc: 0.9835 - val_loss: 0.0444 - val_acc: 0.9835\n",
      "epoch: 2 - val_roc_auc: 0.9868\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 2\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_valid, y_valid))\n",
    "\n",
    "hist = model.fit(X_train, y_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=N_EPOCHS, \n",
    "                 validation_data=(X_valid, y_valid),\n",
    "                 callbacks=[RocAuc])\n",
    "\n",
    "# save trained nnet to disk for later use\n",
    "save_nnet(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score on train set : 0.9938\n",
      "ROC-AUC score on validation set : 0.9868\n"
     ]
    }
   ],
   "source": [
    "# final model evaluation\n",
    "y_train_pred = model.predict(X_train, batch_size=512)\n",
    "train_score = evaluate(y_train, y_train_pred)\n",
    "print(\"ROC-AUC score on train set : {:.4f}\".format(train_score)) \n",
    "\n",
    "y_valid_pred = model.predict(X_valid, batch_size=512)\n",
    "valid_score = evaluate(y_valid, y_valid_pred)\n",
    "print(\"ROC-AUC score on validation set : {:.4f}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_test_pred = model.predict(X_test, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write submission file\n",
    "submission(y_test_pred, id_test, name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Use pre-trained word2vec words for embeddings\n",
    "\n",
    "le github précédent utilise word2vec pour entraîner sur le corpus du problème même, et pas comme source d'embeddings pré entraînés... https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb\n",
    "\n",
    "\"In this subsection, I use word2vec to create word embeddings from the review comments. Word2vec is one algorithm for learning a word embedding from a text corpus.\" --->>> à la base word2vec c'est le réseau d'extraction, on veut récupérer un résultat d'entraînement de référence de cette architecture !\n",
    "\n",
    "Il faut utiliser word2vec entraîné sur Google News, embeddings de taille 300, par Mikolov https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "Post de blog chargeant word2vec pré-entraîné par Mikolov:\n",
    "\n",
    "https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "\n",
    "Lien de téléchargement des embeddings pré entraînés:\n",
    "\n",
    "https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialisation statistique de la matrice d'embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pre-trained word vectors in database       : 3000000\n",
      "Number of our words with a pre-trained embedding     : 24431\n",
      "Percentage of our words with a pre-trained embedding : 81.437%\n"
     ]
    }
   ],
   "source": [
    "# Load Word2Vec Google News pre-trained embeddings\n",
    "EMBEDDING_DIM = 300  # several embeddings sizes depending on source : 25, 50, 100, 200, 300 \n",
    "EMBEDDING_SOURCE = 'word2vec_googlenews'  # {'glove_twitter', 'glove_wikipedia', 'word2vec_googlenews'}\n",
    "\n",
    "embeddings_matrix = load_pretrained_embeddings(tokenizer.word_index, VOCAB_SIZE, EMBEDDING_DIM, EMBEDDING_SOURCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du réseau de Yoon Kim pour word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EMBEDDINGS = True\n",
    "MODEL_NAME = \"embed_LSTM_BIDIR_word2vec_emb300_pretrained_trainableTrue_goodStatInit\"\n",
    "\n",
    "model_google = bidirectional_lstm(sentence_length=SENTENCE_LENGTH, vocab_size=VOCAB_SIZE,\n",
    "                    embedding_dim=EMBEDDING_DIM, embedding_matrix=embeddings_matrix, train_embeddings=TRAIN_EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 2095s 15ms/step - loss: 0.0596 - acc: 0.9798 - val_loss: 0.0454 - val_acc: 0.9838\n",
      "epoch: 1 - val_roc_auc: 0.9799\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 2098s 15ms/step - loss: 0.0414 - acc: 0.9841 - val_loss: 0.0429 - val_acc: 0.9839\n",
      "epoch: 2 - val_roc_auc: 0.9853\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 2\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_valid, y_valid))\n",
    "\n",
    "hist = model_google.fit(X_train, y_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=N_EPOCHS, \n",
    "                 validation_data=(X_valid, y_valid),\n",
    "                 callbacks=[RocAuc])\n",
    "\n",
    "# save trained nnet to disk for later use\n",
    "save_nnet(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score on train set : 0.9927\n",
      "ROC-AUC score on validation set : 0.9853\n"
     ]
    }
   ],
   "source": [
    "# final model evaluation\n",
    "y_train_pred = model_google.predict(X_train, batch_size=512)\n",
    "train_score = evaluate(y_train, y_train_pred)\n",
    "print(\"ROC-AUC score on train set : {:.4f}\".format(train_score)) \n",
    "\n",
    "y_valid_pred = model_google.predict(X_valid, batch_size=512)\n",
    "valid_score = evaluate(y_valid, y_valid_pred)\n",
    "print(\"ROC-AUC score on validation set : {:.4f}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_test_pred = model_google.predict(X_test, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write submission file\n",
    "submission(y_test_pred, id_test, name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. What about Freebase ?\n",
    "\n",
    "https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Use pre-trained GloVe words for embeddings with LSTM model\n",
    "\n",
    "## embedding_matrix avec les embeddings de Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe pre-trained embeddings\n",
    "EMBEDDING_DIM = 200  # several embeddings sizes depending on source : 25, 50, 100, 200, 300 \n",
    "EMBEDDING_SOURCE = 'glove_twitter'  # {'glove_twitter', 'glove_wikipedia', 'word2vec_googlenews'}\n",
    "\n",
    "embeddings_matrix = load_pretrained_embeddings(tokenizer.word_index, VOCAB_SIZE, EMBEDDING_DIM, EMBEDDING_SOURCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du réseau LSTM pour GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EMBEDDINGS = True\n",
    "MODEL_NAME = \"draft_embed_bidirlstm_2fc_EMB_PRETRAINED_GLOVE200t_TWITTER\"\n",
    "\n",
    "model = bidirectional_lstm(sentence_length=SENTENCE_LENGTH, vocab_size=VOCAB_SIZE,\n",
    "                    embedding_dim=EMBEDDING_DIM, embedding_matrix=embeddings_matrix, train_embeddings=TRAIN_EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "  5824/143613 [>.............................] - ETA: 28:16 - loss: 0.1313 - acc: 0.9605"
     ]
    }
   ],
   "source": [
    "# train\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 2\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_valid, y_valid))\n",
    "\n",
    "hist = model.fit(X_train, y_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=N_EPOCHS, \n",
    "                 validation_data=(X_valid, y_valid),\n",
    "                 callbacks=[RocAuc])\n",
    "\n",
    "# save trained nnet to disk for later use\n",
    "save_nnet(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score on train set : 0.9933\n",
      "ROC-AUC score on validation set : 0.9867\n"
     ]
    }
   ],
   "source": [
    "# final model evaluation\n",
    "y_train_pred = model.predict(X_train, batch_size=512)\n",
    "train_score = evaluate(y_train, y_train_pred)\n",
    "print(\"ROC-AUC score on train set : {:.4f}\".format(train_score)) \n",
    "\n",
    "y_valid_pred = model.predict(X_valid, batch_size=512)\n",
    "valid_score = evaluate(y_valid, y_valid_pred)\n",
    "print(\"ROC-AUC score on validation set : {:.4f}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_test_pred = model.predict(X_test, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write submission file\n",
    "submission(y_test_pred, id_test, name=MODEL_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
