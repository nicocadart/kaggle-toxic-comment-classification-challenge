{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan de ce notebook\n",
    "\n",
    "1. Use pre-trained GloVe words for embeddings\n",
    "2. Use pre-trained word2vec words for embeddings\n",
    "3. What about Freebase ?\n",
    "\n",
    "pour standardiser le nom des variables: GloVe _stanford et word2vec _google ?\n",
    "\n",
    "# Nettoyage et conversion numérique des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GlobalMaxPooling1D, Bidirectional, Conv1D, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw string data\n",
    "data_train, y_train_all, data_test, id_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des données (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
      "-------\n",
      "hey man i m really not trying to edit war it s just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info \n"
     ]
    }
   ],
   "source": [
    "params = {'lower': True, \n",
    "          'lemma': False, \n",
    "          'stop_words': False}\n",
    "\n",
    "comment = data_train[2]\n",
    "print(comment)\n",
    "print('-------')\n",
    "print(clean_comment(comment, **params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation: 100%       \n",
      "Transformation: 100%       \n"
     ]
    }
   ],
   "source": [
    "clean_data_train = transform_dataset(data_train, func=clean_comment, kwargs=params)\n",
    "clean_data_test = transform_dataset(data_test, func=clean_comment, kwargs=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization et découpage des données textuelles \n",
    "\n",
    "Conforme au github https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb\n",
    "\n",
    "pour pouvoir encode avec Glove après (ne pas encode directement != Nicolas sur models_testing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert strings to int indexes, \n",
    "# considering only the VOCAB_SIZE most commons words, \n",
    "# and pad the sentences to SENTENCE_LENGTH words\n",
    "VOCAB_SIZE = 30000\n",
    "SENTENCE_LENGTH = 200  # 200 if stop_words deleted, 120 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODING: Fitting vectorizer to data\n",
      "ENCODING: transforming data to numerical\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TokenVectorizer(max_len=SENTENCE_LENGTH, max_features=VOCAB_SIZE)\n",
    "\n",
    "# X_train_all, X_test = encode(data_train, data_test, vectorizer=tokens_vectorizer)\n",
    "X_train_all, X_test = encode(clean_data_train, clean_data_test, vectorizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_VALID_RATIO = 0.10\n",
    "SPLIT_RANDOM_SEED = 0  # TODO : check split because of unbalanced classes\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all, \n",
    "                                                      test_size=SPLIT_VALID_RATIO,\n",
    "                                                      random_state=SPLIT_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Use pre-trained GloVe words for embeddings\n",
    "\n",
    "https://medium.com/@sabber/classifying-yelp-review-comments-using-cnn-lstm-and-pre-trained-glove-word-embeddings-part-3-53fcea9a17fa\n",
    "\n",
    "https://github.com/msahamed/yelp_comments_classification_nlp\n",
    "\n",
    "https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb\n",
    "\n",
    "## embedding_matrix avec les embeddings de Glove\n",
    "\n",
    "several possibilities of pre-training/embeddings vector sizes for GloVe, see:\n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "dans Mikolov c'est du 300 pour la taille des embeddings https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe pre-trained embeddings\n",
    "EMBEDDING_DIM = 200  # several embeddings sizes possible with Glove : 50, 100, 200, 300 \n",
    "GLOVE_EMBEDDINGS = 'embeddings/glove.6B.{}d.txt'.format(EMBEDDING_DIM)\n",
    "\n",
    "embeddings_index = dict()\n",
    "with open(GLOVE_EMBEDDINGS) as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        #print(coefs.shape) # le code est bien compatible avec le format renvoyé par word2vec\n",
    "        embeddings_index[word] = coefs\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a weight matrix for words in training docs\n",
    "\n",
    "# get mean and std values of pre-trained embeddings\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = np.mean(all_embs, axis=0), np.std(all_embs, axis=0)\n",
    "\n",
    "# init matrix to embeddings distribution\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (VOCAB_SIZE, EMBEDDING_DIM))\n",
    "# loop on words in our documents\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    # if word isn't enough used in documents, ignore it\n",
    "    if index >= VOCAB_SIZE: \n",
    "        continue\n",
    "    # otherwise, fill embedding matrix with pre-trained vector corresponding to this word\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index, :] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du réseau de Yoon Kim pour GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FILTERS = 100\n",
    "MODEL_NAME = \"embed_conv_fc_GLOVE200t\"\n",
    "\n",
    "# input\n",
    "inp = Input(shape=(SENTENCE_LENGTH, ))\n",
    "# embedding\n",
    "emb = Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=SENTENCE_LENGTH, \n",
    "                weights=[embedding_matrix], trainable=True)(inp)\n",
    "\n",
    "# Specify each convolution layer and their kernel siz i.e. n-grams \n",
    "conv_3 = Conv1D(filters=N_FILTERS, kernel_size=3, activation='relu')(emb)\n",
    "pool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "\n",
    "conv_4 = Conv1D(filters=N_FILTERS, kernel_size=4, activation='relu')(emb)\n",
    "pool_4 = GlobalMaxPooling1D()(conv_4)\n",
    "\n",
    "conv_5 = Conv1D(filters=N_FILTERS, kernel_size=5, activation='relu')(emb)\n",
    "pool_5 = GlobalMaxPooling1D()(conv_5)\n",
    "\n",
    "# Gather all convolution layers\n",
    "x = concatenate([pool_3, pool_4, pool_5], axis=1)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outp = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "# # load pre-trained model from disk\n",
    "# model = load_nnet(MODEL_NAME)\n",
    "\n",
    "model = Model(inputs=inp, outputs=outp)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1203s 8ms/step - loss: 0.0544 - acc: 0.9800 - val_loss: 0.0443 - val_acc: 0.9831\n",
      "epoch: 1 - val_roc_auc: 0.9846\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1242s 9ms/step - loss: 0.0407 - acc: 0.9838 - val_loss: 0.0445 - val_acc: 0.9831\n",
      "epoch: 2 - val_roc_auc: 0.9858\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 2\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_valid, y_valid))\n",
    "\n",
    "hist = model.fit(X_train, y_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=N_EPOCHS, \n",
    "                 validation_data=(X_valid, y_valid),\n",
    "                 callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained nnet to disk for later use\n",
    "save_nnet(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score on train set : 0.9944\n",
      "ROC-AUC score on validation set : 0.9858\n"
     ]
    }
   ],
   "source": [
    "# final model evaluation\n",
    "y_train_pred = model.predict(X_train, batch_size=512)\n",
    "train_score = evaluate(y_train, y_train_pred)\n",
    "print(\"ROC-AUC score on train set : {:.4f}\".format(train_score)) \n",
    "\n",
    "y_valid_pred = model.predict(X_valid, batch_size=512)\n",
    "valid_score = evaluate(y_valid, y_valid_pred)\n",
    "print(\"ROC-AUC score on validation set : {:.4f}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_test_pred = model.predict(X_test, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write submission file\n",
    "submission(y_test_pred, id_test, name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Use pre-trained word2vec words for embeddings\n",
    "\n",
    "le github précédent utilise word2vec pour entraîner sur le corpus du problème même, et pas comme source d'embeddings pré entraînés... https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb\n",
    "\n",
    "\"In this subsection, I use word2vec to create word embeddings from the review comments. Word2vec is one algorithm for learning a word embedding from a text corpus.\" --->>> à la base word2vec c'est le réseau d'extraction, on veut récupérer un résultat d'entraînement de référence de cette architecture !\n",
    "\n",
    "Il faut utiliser word2vec entraîné sur Google News, embeddings de taille 300, par Mikolov https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "Post de blog chargeant word2vec pré-entraîné par Mikolov:\n",
    "\n",
    "https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "\n",
    "Lien de téléchargement des embeddings pré entraînés:\n",
    "\n",
    "https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "filename = 'GoogleNews-vectors-negative300.bin/data'\n",
    "google_model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in pre trained word2vec: 3000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbauw/Documents/Académique/M2_AIC/OPT1_DataCamp/mars_craters/mars_env/lib/python3.5/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "google_model_words = list(google_model.wv.vocab)\n",
    "print(\"Number of words in pre trained word2vec:\",len(google_model_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(google_model['fuck']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model['fuck'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding_matrix avec les embeddings de google news word2vec\n",
    "\n",
    "on s'inspire de la construction de la matrice pour GloVe, \n",
    "ici pas besoin de construire le dictionnaire à partir du modèle\n",
    "\n",
    "notons que le tokenizer est déjà initialisé et reste le même"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes inchangées:\n",
    "# VOCAB_SIZE = 30000\n",
    "# SENTENCE_LENGTH = 200  # 200 if stop_words deleted, 120 otherwise\n",
    "\n",
    "# ONLY ONE embedding size possible with google news trained word2vec\n",
    "EMBEDDING_DIM_GOOGLE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix_google = np.zeros((VOCAB_SIZE, EMBEDDING_DIM_GOOGLE))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > VOCAB_SIZE - 1: # détermine à quel point on s'intéresse aux mots moins importants d'après Glove\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector_google = google_model[word] # on va chercher le mot dans word2vec embeddings\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix_google[index] = embedding_vector_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_google.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du réseau de Yoon Kim pour word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FILTERS = 100\n",
    "MODEL_NAME_GOOGLE = \"embed_conv_fc_GOOGLE\"\n",
    "\n",
    "# input\n",
    "inp = Input(shape=(SENTENCE_LENGTH, ))\n",
    "# embedding\n",
    "emb = Embedding(VOCAB_SIZE, EMBEDDING_DIM_GOOGLE,input_length=SENTENCE_LENGTH,weights=[embedding_matrix_google], trainable=False)(inp)\n",
    "\n",
    "# Specify each convolution layer and their kernel siz i.e. n-grams \n",
    "conv_3 = Conv1D(filters=N_FILTERS, kernel_size=3, activation='relu')(emb)\n",
    "pool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "\n",
    "conv_4 = Conv1D(filters=N_FILTERS, kernel_size=4, activation='relu')(emb)\n",
    "pool_4 = GlobalMaxPooling1D()(conv_4)\n",
    "\n",
    "conv_5 = Conv1D(filters=N_FILTERS, kernel_size=5, activation='relu')(emb)\n",
    "pool_5 = GlobalMaxPooling1D()(conv_5)\n",
    "\n",
    "# Gather all convolution layers\n",
    "x = concatenate([pool_3, pool_4, pool_5], axis=1)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outp = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "# # load pre-trained model from disk\n",
    "# model = load_nnet(MODEL_NAME)\n",
    "\n",
    "model_google = Model(inputs=inp, outputs=outp)\n",
    "model_google.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1552s 11ms/step - loss: 0.2890 - acc: 0.9634 - val_loss: 0.1603 - val_acc: 0.9629\n",
      "epoch: 1 - val_roc_auc: 0.5000\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1083s 8ms/step - loss: 0.1464 - acc: 0.9634 - val_loss: 0.1431 - val_acc: 0.9629\n",
      "epoch: 2 - val_roc_auc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 2\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_valid, y_valid))\n",
    "\n",
    "hist = model_google.fit(X_train, y_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=N_EPOCHS, \n",
    "                 validation_data=(X_valid, y_valid),\n",
    "                 callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained nnet to disk for later use\n",
    "save_nnet(model_google, MODEL_NAME_GOOGLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score on train set : 0.5000\n",
      "ROC-AUC score on validation set : 0.5000\n"
     ]
    }
   ],
   "source": [
    "# final model evaluation\n",
    "y_train_pred = model_google.predict(X_train, batch_size=512)\n",
    "train_score = evaluate(y_train, y_train_pred)\n",
    "print(\"ROC-AUC score on train set : {:.4f}\".format(train_score)) \n",
    "\n",
    "y_valid_pred = model_google.predict(X_valid, batch_size=512)\n",
    "valid_score = evaluate(y_valid, y_valid_pred)\n",
    "print(\"ROC-AUC score on validation set : {:.4f}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_test_pred = model_google.predict(X_test, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write submission file\n",
    "submission(y_test_pred, id_test, name=MODEL_NAME_GOOGLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. What about Freebase ?\n",
    "\n",
    "https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
