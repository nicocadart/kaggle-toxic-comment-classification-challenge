{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan de ce notebook\n",
    "\n",
    "1. Use pre-trained GloVe words for embeddings\n",
    "2. Use pre-trained word2vec words for embeddings\n",
    "3. What about Freebase ?\n",
    "4. Use pre-trained GloVe words for embeddings with LSTM model\n",
    "\n",
    "pour standardiser le nom des variables: GloVe _stanford et word2vec _google ?\n",
    "\n",
    "# Nettoyage et conversion numérique des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from tools import *\n",
    "from embeddings import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw string data\n",
    "data_train, y_train_all, data_test, id_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des données (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
      "-------\n",
      "hey man i m really not trying to edit war it s just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info \n"
     ]
    }
   ],
   "source": [
    "params = {'lower': True, \n",
    "          'lemma': False, \n",
    "          'stop_words': False}\n",
    "\n",
    "comment = data_train[2]\n",
    "print(comment)\n",
    "print('-------')\n",
    "print(clean_comment(comment, **params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation: 100%       \n",
      "Transformation: 100%       \n"
     ]
    }
   ],
   "source": [
    "clean_data_train = transform_dataset(data_train, func=clean_comment, kwargs=params)\n",
    "clean_data_test = transform_dataset(data_test, func=clean_comment, kwargs=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization et découpage des données textuelles \n",
    "\n",
    "Conforme au github https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb\n",
    "\n",
    "pour pouvoir encode avec Glove après (ne pas encode directement != Nicolas sur models_testing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert strings to int indexes, \n",
    "# considering only the VOCAB_SIZE most commons words, \n",
    "# and pad the sentences to SENTENCE_LENGTH words\n",
    "VOCAB_SIZE = 30000\n",
    "SENTENCE_LENGTH = 200  # 200 if stop_words deleted, 120 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODING: Fitting vectorizer to data\n",
      "ENCODING: transforming data to numerical\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TokenVectorizer(max_len=SENTENCE_LENGTH, max_features=VOCAB_SIZE)\n",
    "\n",
    "# X_train_all, X_test = encode(data_train, data_test, vectorizer=tokens_vectorizer)\n",
    "X_train_all, X_test = encode(clean_data_train, clean_data_test, vectorizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_VALID_RATIO = 0.10\n",
    "SPLIT_RANDOM_SEED = 0  # TODO : check split because of unbalanced classes\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all, \n",
    "                                                      test_size=SPLIT_VALID_RATIO,\n",
    "                                                      random_state=SPLIT_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Use pre-trained GloVe words for embeddings\n",
    "\n",
    "https://medium.com/@sabber/classifying-yelp-review-comments-using-cnn-lstm-and-pre-trained-glove-word-embeddings-part-3-53fcea9a17fa\n",
    "\n",
    "https://github.com/msahamed/yelp_comments_classification_nlp\n",
    "\n",
    "https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb\n",
    "\n",
    "## embedding_matrix avec les embeddings de Glove\n",
    "\n",
    "several possibilities of pre-training/embeddings vector sizes for GloVe, see:\n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "dans Mikolov c'est du 300 pour la taille des embeddings https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "Number of words with a GloVe embedding: 27288\n",
      "Percentage of words with a GloVe embedding: 0.9096\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe pre-trained embeddings\n",
    "EMBEDDING_DIM = 200  # several embeddings sizes possible with Glove : 50, 100, 200, 300 \n",
    "GLOVE_EMBEDDINGS = 'glove.6B/glove.6B.{}d.txt'.format(EMBEDDING_DIM)\n",
    "\n",
    "Glove_embeddings_matrix = load_GloVe_embeddings(EMBEDDING_DIM,VOCAB_SIZE,\\\n",
    "                                               GLOVE_EMBEDDINGS,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du réseau de Yoon Kim pour GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "YoonKim() missing 1 required positional argument: 'trainableBool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2d6108e6dd7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"embed_conv_fc_GLOVE_emb200_pretrained_trainableFalse_goodStatInit\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYoonKim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSENTENCE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGlove_embeddings_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_FILTERS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: YoonKim() missing 1 required positional argument: 'trainableBool'"
     ]
    }
   ],
   "source": [
    "N_FILTERS = 100\n",
    "MODEL_NAME = \"embed_conv_fc_GLOVE_emb200_pretrained_trainableFalse_goodStatInit\"\n",
    "\n",
    "model = YoonKim(SENTENCE_LENGTH,EMBEDDING_DIM,Glove_embeddings_matrix,N_FILTERS,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1309s 9ms/step - loss: 0.0558 - acc: 0.9796 - val_loss: 0.0432 - val_acc: 0.9835\n",
      "epoch: 1 - val_roc_auc: 0.9856\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1287s 9ms/step - loss: 0.0411 - acc: 0.9837 - val_loss: 0.0450 - val_acc: 0.9828\n",
      "epoch: 2 - val_roc_auc: 0.9868\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 2\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_valid, y_valid))\n",
    "\n",
    "hist = model.fit(X_train, y_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=N_EPOCHS, \n",
    "                 validation_data=(X_valid, y_valid),\n",
    "                 callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained nnet to disk for later use\n",
    "save_nnet(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score on train set : 0.9940\n",
      "ROC-AUC score on validation set : 0.9868\n"
     ]
    }
   ],
   "source": [
    "# final model evaluation\n",
    "y_train_pred = model.predict(X_train, batch_size=512)\n",
    "train_score = evaluate(y_train, y_train_pred)\n",
    "print(\"ROC-AUC score on train set : {:.4f}\".format(train_score)) \n",
    "\n",
    "y_valid_pred = model.predict(X_valid, batch_size=512)\n",
    "valid_score = evaluate(y_valid, y_valid_pred)\n",
    "print(\"ROC-AUC score on validation set : {:.4f}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_test_pred = model.predict(X_test, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write submission file\n",
    "submission(y_test_pred, id_test, name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Use pre-trained word2vec words for embeddings\n",
    "\n",
    "le github précédent utilise word2vec pour entraîner sur le corpus du problème même, et pas comme source d'embeddings pré entraînés... https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb\n",
    "\n",
    "\"In this subsection, I use word2vec to create word embeddings from the review comments. Word2vec is one algorithm for learning a word embedding from a text corpus.\" --->>> à la base word2vec c'est le réseau d'extraction, on veut récupérer un résultat d'entraînement de référence de cette architecture !\n",
    "\n",
    "Il faut utiliser word2vec entraîné sur Google News, embeddings de taille 300, par Mikolov https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "Post de blog chargeant word2vec pré-entraîné par Mikolov:\n",
    "\n",
    "https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "\n",
    "Lien de téléchargement des embeddings pré entraînés:\n",
    "\n",
    "https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialisation statistique de la matrice d'embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY ONE embedding size possible with google news trained word2vec\n",
    "EMBEDDING_DIM_GOOGLE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert strings to int indexes, \n",
    "# considering only the VOCAB_SIZE most commons words, \n",
    "# and pad the sentences to SENTENCE_LENGTH words\n",
    "VOCAB_SIZE = 30000\n",
    "SENTENCE_LENGTH = 200  # 200 if stop_words deleted, 120 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in pre trained word2vec: 3000000\n",
      "Loaded 3000000 word vectors.\n",
      "Embeddings index loaded, now initializing embeddings matrix with statistical caracteristics\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-613bb57631db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m embeddings_matrix_init = init_word2vec_embeddings(google_embeddings_index, \\\n\u001b[0;32m---> 10\u001b[0;31m                                                   EMBEDDING_DIM_GOOGLE,VOCAB_SIZE)\n\u001b[0m",
      "\u001b[0;32m~/Documents/Académique/M2_AIC/OPT7_DeepStruct/kaggle-toxic-comment-classification-challenge/embeddings.py\u001b[0m in \u001b[0;36minit_word2vec_embeddings\u001b[0;34m(embeddings_index_google, emb_dim, vocab_size)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mall_embs_google\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_index_google\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0memb_mean_google\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_std_google\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_embs_google\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                                         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_embs_google\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mall_embs_google\u001b[0m \u001b[0;31m# FREE RAM ! END MEMORY GULAG !\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Académique/M2_AIC/OPT1_DataCamp/mars_craters/mars_env/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mstd\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m     return _methods._std(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\u001b[0;32m-> 3075\u001b[0;31m                          **kwargs)\n\u001b[0m\u001b[1;32m   3076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Académique/M2_AIC/OPT1_DataCamp/mars_craters/mars_env/lib/python3.5/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_std\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_std\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\u001b[0;32m--> 135\u001b[0;31m                keepdims=keepdims)\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Académique/M2_AIC/OPT1_DataCamp/mars_craters/mars_env/lib/python3.5/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Note that x may not be inexact and that we need it to be an array,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# not a scalar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0marrmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplexfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconjugate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from embeddings import *\n",
    "from models import *\n",
    "\n",
    "filename = 'GoogleNews-vectors-negative300.bin/data'\n",
    "google_embeddings_index = load_word2vec_embeddings_index(filename)\n",
    "\n",
    "print(\"Embeddings index loaded, now initializing embeddings matrix with statistical caracteristics\")\n",
    "\n",
    "embeddings_matrix_init = init_word2vec_embeddings(google_embeddings_index, \\\n",
    "                                                  EMBEDDING_DIM_GOOGLE,VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word2vec pretrained embeddings initialized matrix to .csv to avoid\n",
    "# RAM killing computations\n",
    "np.save('./data/init_embedding_matrix_google',embedding_matrix_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prise en compte des embeddings des mots de notre corpus dans la matrice d'embeddings\n",
    "\n",
    "## + chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw string data\n",
    "data_train, y_train_all, data_test, id_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation: 100%       \n",
      "Transformation: 100%       \n"
     ]
    }
   ],
   "source": [
    "params = {'lower': True, \n",
    "          'lemma': False, \n",
    "          'stop_words': False}\n",
    "\n",
    "clean_data_train = transform_dataset(data_train, func=clean_comment, kwargs=params)\n",
    "clean_data_test = transform_dataset(data_test, func=clean_comment, kwargs=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODING: Fitting vectorizer to data\n",
      "ENCODING: transforming data to numerical\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TokenVectorizer(max_len=SENTENCE_LENGTH, max_features=VOCAB_SIZE)\n",
    "\n",
    "# X_train_all, X_test = encode(data_train, data_test, vectorizer=tokens_vectorizer)\n",
    "X_train_all, X_test = encode(clean_data_train, clean_data_test, vectorizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_VALID_RATIO = 0.10\n",
    "SPLIT_RANDOM_SEED = 0  # TODO : check split because of unbalanced classes\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all, \n",
    "                                                      test_size=SPLIT_VALID_RATIO,\n",
    "                                                      random_state=SPLIT_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with a word2vec embedding: 24533\n",
      "Percentage of words with a word2vec embedding: 0.8177666666666666\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_google = load_word2vec_embeddings(embedding_matrix_google,\\\n",
    "                                                   embeddings_index_google,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du réseau de Yoon Kim pour word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FILTERS = 100\n",
    "MODEL_NAME_GOOGLE = \"embed_conv_fc_GOOGLE\"\n",
    "\n",
    "# input\n",
    "inp = Input(shape=(SENTENCE_LENGTH, ))\n",
    "# embedding\n",
    "emb = Embedding(VOCAB_SIZE, EMBEDDING_DIM_GOOGLE,input_length=SENTENCE_LENGTH,weights=[embedding_matrix_google], trainable=False)(inp)\n",
    "\n",
    "# Specify each convolution layer and their kernel siz i.e. n-grams \n",
    "conv_3 = Conv1D(filters=N_FILTERS, kernel_size=3, activation='relu')(emb)\n",
    "pool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "\n",
    "conv_4 = Conv1D(filters=N_FILTERS, kernel_size=4, activation='relu')(emb)\n",
    "pool_4 = GlobalMaxPooling1D()(conv_4)\n",
    "\n",
    "conv_5 = Conv1D(filters=N_FILTERS, kernel_size=5, activation='relu')(emb)\n",
    "pool_5 = GlobalMaxPooling1D()(conv_5)\n",
    "\n",
    "# Gather all convolution layers\n",
    "x = concatenate([pool_3, pool_4, pool_5], axis=1)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outp = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "# # load pre-trained model from disk\n",
    "# model = load_nnet(MODEL_NAME)\n",
    "\n",
    "model_google = Model(inputs=inp, outputs=outp)\n",
    "model_google.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1107s 8ms/step - loss: 0.0554 - acc: 0.9798 - val_loss: 0.0458 - val_acc: 0.9826\n",
      "epoch: 1 - val_roc_auc: 0.9857\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1086s 8ms/step - loss: 0.0445 - acc: 0.9829 - val_loss: 0.0442 - val_acc: 0.9830\n",
      "epoch: 2 - val_roc_auc: 0.9867\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 2\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_valid, y_valid))\n",
    "\n",
    "hist = model_google.fit(X_train, y_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=N_EPOCHS, \n",
    "                 validation_data=(X_valid, y_valid),\n",
    "                 callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained nnet to disk for later use\n",
    "save_nnet(model_google, MODEL_NAME_GOOGLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score on train set : 0.9921\n",
      "ROC-AUC score on validation set : 0.9867\n"
     ]
    }
   ],
   "source": [
    "# final model evaluation\n",
    "y_train_pred = model_google.predict(X_train, batch_size=512)\n",
    "train_score = evaluate(y_train, y_train_pred)\n",
    "print(\"ROC-AUC score on train set : {:.4f}\".format(train_score)) \n",
    "\n",
    "y_valid_pred = model_google.predict(X_valid, batch_size=512)\n",
    "valid_score = evaluate(y_valid, y_valid_pred)\n",
    "print(\"ROC-AUC score on validation set : {:.4f}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_test_pred = model_google.predict(X_test, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write submission file\n",
    "submission(y_test_pred, id_test, name=MODEL_NAME_GOOGLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. What about Freebase ?\n",
    "\n",
    "https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Use pre-trained GloVe words for embeddings with LSTM model\n",
    "\n",
    "## embedding_matrix avec les embeddings de Glove\n",
    "\n",
    "copy of code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe pre-trained embeddings\n",
    "EMBEDDING_DIM = 200  # several embeddings sizes possible with Glove : 50, 100, 200, 300 \n",
    "GLOVE_EMBEDDINGS = 'glove.6B/glove.6B.{}d.txt'.format(EMBEDDING_DIM)\n",
    "\n",
    "embeddings_index = dict()\n",
    "with open(GLOVE_EMBEDDINGS) as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        #print(coefs.shape) # le code est bien compatible avec le format renvoyé par word2vec\n",
    "        embeddings_index[word] = coefs\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a weight matrix for words in training docs\n",
    "\n",
    "# get mean and std values of pre-trained embeddings\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "# print(all_embs.shape)\n",
    "emb_mean, emb_std = np.mean(all_embs, axis=0), np.std(all_embs, axis=0)\n",
    "# print(emb_mean)\n",
    "# print(emb_std) # les prints sont là pour évaluer comment faire l'initialisation arbitraire pour word2vec\n",
    "\n",
    "# init matrix to embeddings distribution\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (VOCAB_SIZE, EMBEDDING_DIM))\n",
    "# print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with a GloVe embedding: 27288\n",
      "Percentage of words with a GloVe embedding: 0.9096\n"
     ]
    }
   ],
   "source": [
    "# loop on words in our documents\n",
    "\n",
    "wordWithGloveEmb = 0\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    # if word isn't enough used in documents, ignore it\n",
    "    if index >= VOCAB_SIZE: \n",
    "        continue\n",
    "    # otherwise, fill embedding matrix with pre-trained vector corresponding to this word\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        wordWithGloveEmb += 1\n",
    "        embedding_matrix[index, :] = embedding_vector\n",
    "        \n",
    "print(\"Number of words with a GloVe embedding:\",wordWithGloveEmb)\n",
    "print(\"Percentage of words with a GloVe embedding:\",wordWithGloveEmb/VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du réseau LSTM pour GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"draft_embed_bidirlstm_2fc_EMB_PRETRAINED_GLOVE200t\"\n",
    "\n",
    "# input\n",
    "inp = Input(shape=(SENTENCE_LENGTH, ))\n",
    "# embedding\n",
    "x = Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=SENTENCE_LENGTH, \n",
    "                weights=[embedding_matrix], trainable=True)(inp)\n",
    "# LSTM\n",
    "x = Bidirectional(LSTM(60, return_sequences=True,name='lstm_layer'))(x)\n",
    "# max pooling 1D\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "# dropout 1\n",
    "x = Dropout(0.1)(x)\n",
    "# dense 1\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "# dropout 2\n",
    "x = Dropout(0.1)(x)\n",
    "# dense 1\n",
    "outp = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "# # load pre-trained model from disk\n",
    "# model = load_nnet(MODEL_NAME)\n",
    "\n",
    "model = Model(inputs=inp, outputs=outp)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1968s 14ms/step - loss: 0.0540 - acc: 0.9805 - val_loss: 0.0436 - val_acc: 0.9834\n",
      "epoch: 1 - val_roc_auc: 0.9854\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1878s 13ms/step - loss: 0.0395 - acc: 0.9845 - val_loss: 0.0418 - val_acc: 0.9839\n",
      "epoch: 2 - val_roc_auc: 0.9867\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 2\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_valid, y_valid))\n",
    "\n",
    "hist = model.fit(X_train, y_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=N_EPOCHS, \n",
    "                 validation_data=(X_valid, y_valid),\n",
    "                 callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained nnet to disk for later use\n",
    "save_nnet(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score on train set : 0.9933\n",
      "ROC-AUC score on validation set : 0.9867\n"
     ]
    }
   ],
   "source": [
    "# final model evaluation\n",
    "y_train_pred = model.predict(X_train, batch_size=512)\n",
    "train_score = evaluate(y_train, y_train_pred)\n",
    "print(\"ROC-AUC score on train set : {:.4f}\".format(train_score)) \n",
    "\n",
    "y_valid_pred = model.predict(X_valid, batch_size=512)\n",
    "valid_score = evaluate(y_valid, y_valid_pred)\n",
    "print(\"ROC-AUC score on validation set : {:.4f}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_test_pred = model.predict(X_test, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write submission file\n",
    "submission(y_test_pred, id_test, name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mars_env",
   "language": "python",
   "name": "mars_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
