{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan de ce notebook\n",
    "\n",
    "1. Use pre-trained GloVe words for embeddings\n",
    "2. Use pre-trained word2vec words for embeddings\n",
    "3. What about Freebase ?\n",
    "4. Use pre-trained GloVe words for embeddings with LSTM model\n",
    "\n",
    "pour standardiser le nom des variables: GloVe _stanford et word2vec _google ?\n",
    "\n",
    "# Nettoyage et conversion numérique des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from tools import *\n",
    "from embeddings import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw string data\n",
    "data_train, y_train_all, data_test, id_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des données (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
      "-------\n",
      "hey man i m really not trying to edit war it s just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info \n"
     ]
    }
   ],
   "source": [
    "params = {'lower': True, \n",
    "          'lemma': False, \n",
    "          'stop_words': False}\n",
    "\n",
    "comment = data_train[2]\n",
    "print(comment)\n",
    "print('-------')\n",
    "print(clean_comment(comment, **params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation: 100%       \n",
      "Transformation: 100%       \n"
     ]
    }
   ],
   "source": [
    "clean_data_train = transform_dataset(data_train, func=clean_comment, kwargs=params)\n",
    "clean_data_test = transform_dataset(data_test, func=clean_comment, kwargs=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization et découpage des données textuelles \n",
    "\n",
    "Conforme au github https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb\n",
    "\n",
    "pour pouvoir encode avec Glove après (ne pas encode directement != Nicolas sur models_testing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert strings to int indexes, \n",
    "# considering only the VOCAB_SIZE most commons words, \n",
    "# and pad the sentences to SENTENCE_LENGTH words\n",
    "VOCAB_SIZE = 30000\n",
    "SENTENCE_LENGTH = 200  # 200 if stop_words deleted, 120 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODING: Fitting vectorizer to data\n",
      "ENCODING: transforming data to numerical\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TokenVectorizer(max_len=SENTENCE_LENGTH, max_features=VOCAB_SIZE)\n",
    "\n",
    "# X_train_all, X_test = encode(data_train, data_test, vectorizer=tokens_vectorizer)\n",
    "X_train_all, X_test = encode(clean_data_train, clean_data_test, vectorizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_VALID_RATIO = 0.10\n",
    "SPLIT_RANDOM_SEED = 0  # TODO : check split because of unbalanced classes\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all, \n",
    "                                                      test_size=SPLIT_VALID_RATIO,\n",
    "                                                      random_state=SPLIT_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Use pre-trained GloVe words for embeddings\n",
    "\n",
    "https://medium.com/@sabber/classifying-yelp-review-comments-using-cnn-lstm-and-pre-trained-glove-word-embeddings-part-3-53fcea9a17fa\n",
    "\n",
    "https://github.com/msahamed/yelp_comments_classification_nlp\n",
    "\n",
    "https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb\n",
    "\n",
    "## embedding_matrix avec les embeddings de Glove\n",
    "\n",
    "several possibilities of pre-training/embeddings vector sizes for GloVe, see:\n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "dans Mikolov c'est du 300 pour la taille des embeddings https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "Number of words with a GloVe embedding: 27288\n",
      "Percentage of words with a GloVe embedding: 0.9096\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe pre-trained embeddings\n",
    "EMBEDDING_DIM = 200  # several embeddings sizes possible with Glove : 50, 100, 200, 300 \n",
    "GLOVE_EMBEDDINGS = 'glove.6B/glove.6B.{}d.txt'.format(EMBEDDING_DIM)\n",
    "\n",
    "Glove_embeddings_matrix = load_glove_embeddings(EMBEDDING_DIM,VOCAB_SIZE,\\\n",
    "                                               GLOVE_EMBEDDINGS,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du réseau de Yoon Kim pour GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FILTERS = 100\n",
    "MODEL_NAME = \"embed_conv_fc_GLOVE_emb200_pretrained_trainableTrue_goodStatInit\"\n",
    "\n",
    "model = YoonKim(SENTENCE_LENGTH,VOCAB_SIZE,EMBEDDING_DIM,Glove_embeddings_matrix,N_FILTERS,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1312s 9ms/step - loss: 0.0553 - acc: 0.9800 - val_loss: 0.0451 - val_acc: 0.9826\n",
      "epoch: 1 - val_roc_auc: 0.9849\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1253s 9ms/step - loss: 0.0413 - acc: 0.9835 - val_loss: 0.0444 - val_acc: 0.9835\n",
      "epoch: 2 - val_roc_auc: 0.9868\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 2\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_valid, y_valid))\n",
    "\n",
    "hist = model.fit(X_train, y_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=N_EPOCHS, \n",
    "                 validation_data=(X_valid, y_valid),\n",
    "                 callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained nnet to disk for later use\n",
    "save_nnet(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score on train set : 0.9938\n",
      "ROC-AUC score on validation set : 0.9868\n"
     ]
    }
   ],
   "source": [
    "# final model evaluation\n",
    "y_train_pred = model.predict(X_train, batch_size=512)\n",
    "train_score = evaluate(y_train, y_train_pred)\n",
    "print(\"ROC-AUC score on train set : {:.4f}\".format(train_score)) \n",
    "\n",
    "y_valid_pred = model.predict(X_valid, batch_size=512)\n",
    "valid_score = evaluate(y_valid, y_valid_pred)\n",
    "print(\"ROC-AUC score on validation set : {:.4f}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_test_pred = model.predict(X_test, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write submission file\n",
    "submission(y_test_pred, id_test, name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Use pre-trained word2vec words for embeddings\n",
    "\n",
    "le github précédent utilise word2vec pour entraîner sur le corpus du problème même, et pas comme source d'embeddings pré entraînés... https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb\n",
    "\n",
    "\"In this subsection, I use word2vec to create word embeddings from the review comments. Word2vec is one algorithm for learning a word embedding from a text corpus.\" --->>> à la base word2vec c'est le réseau d'extraction, on veut récupérer un résultat d'entraînement de référence de cette architecture !\n",
    "\n",
    "Il faut utiliser word2vec entraîné sur Google News, embeddings de taille 300, par Mikolov https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "Post de blog chargeant word2vec pré-entraîné par Mikolov:\n",
    "\n",
    "https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "\n",
    "Lien de téléchargement des embeddings pré entraînés:\n",
    "\n",
    "https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialisation statistique de la matrice d'embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY ONE embedding size possible with google news trained word2vec\n",
    "EMBEDDING_DIM_GOOGLE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert strings to int indexes, \n",
    "# considering only the VOCAB_SIZE most commons words, \n",
    "# and pad the sentences to SENTENCE_LENGTH words\n",
    "VOCAB_SIZE = 30000\n",
    "SENTENCE_LENGTH = 200  # 200 if stop_words deleted, 120 otherwise\n",
    "filename = 'GoogleNews-vectors-negative300.bin/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-613bb57631db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'GoogleNews-vectors-negative300.bin/data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgoogle_embeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_word2vec_embeddings_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Embeddings index loaded, now initializing embeddings matrix with statistical caracteristics\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Académique/M2_AIC/OPT7_DeepStruct/kaggle-toxic-comment-classification-challenge/embeddings.py\u001b[0m in \u001b[0;36mload_word2vec_embeddings_index\u001b[0;34m(pathToEmbFile)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \"\"\"\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mgoogle_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathToEmbFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mgoogle_model_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoogle_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of words in pre trained word2vec:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoogle_model_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Académique/M2_AIC/OPT1_DataCamp/mars_craters/mars_env/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Académique/M2_AIC/OPT1_DataCamp/mars_craters/mars_env/lib/python3.5/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline_no\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Académique/M2_AIC/OPT1_DataCamp/mars_craters/mars_env/lib/python3.5/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36madd_word\u001b[0;34m(word, weights)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;31m# most common scenario: no vocab file given. just make up some bogus counts, in descending order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mword_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;31m# use count from the vocab file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from embeddings import *\n",
    "from models import *\n",
    "\n",
    "google_embeddings_index = load_word2vec_embeddings_index(filename)\n",
    "\n",
    "print(\"Embeddings index loaded, now initializing embeddings matrix with statistical caracteristics\")\n",
    "\n",
    "embeddings_matrix_init = init_word2vec_embeddings(google_embeddings_index, \\\n",
    "                                                  EMBEDDING_DIM_GOOGLE,VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word2vec pretrained embeddings initialized matrix to .csv to avoid\n",
    "# RAM killing computations\n",
    "np.save('./data/init_embedding_matrix_google',embeddings_matrix_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prise en compte des embeddings des mots de notre corpus dans la matrice d'embeddings\n",
    "\n",
    "## + chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix_init = np.load('./data/init_embedding_matrix_google.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw string data\n",
    "data_train, y_train_all, data_test, id_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation: 100%       \n",
      "Transformation: 100%       \n"
     ]
    }
   ],
   "source": [
    "params = {'lower': True, \n",
    "          'lemma': False, \n",
    "          'stop_words': False}\n",
    "\n",
    "clean_data_train = transform_dataset(data_train, func=clean_comment, kwargs=params)\n",
    "clean_data_test = transform_dataset(data_test, func=clean_comment, kwargs=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODING: Fitting vectorizer to data\n",
      "ENCODING: transforming data to numerical\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TokenVectorizer(max_len=SENTENCE_LENGTH, max_features=VOCAB_SIZE)\n",
    "\n",
    "# X_train_all, X_test = encode(data_train, data_test, vectorizer=tokens_vectorizer)\n",
    "X_train_all, X_test = encode(clean_data_train, clean_data_test, vectorizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_VALID_RATIO = 0.10\n",
    "SPLIT_RANDOM_SEED = 0  # TODO : check split because of unbalanced classes\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all, \n",
    "                                                      test_size=SPLIT_VALID_RATIO,\n",
    "                                                      random_state=SPLIT_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in pre trained word2vec: 3000000\n",
      "Loaded 3000000 word vectors.\n",
      "now update embeddings associated with words in our corpus\n",
      "Number of words with a word2vec embedding: 24533\n",
      "Percentage of words with a word2vec embedding: 0.8177666666666666\n"
     ]
    }
   ],
   "source": [
    "embeddings_index_google = load_word2vec_embeddings_index(filename)\n",
    "\n",
    "print(\"now update embeddings associated with words in our corpus\")\n",
    "\n",
    "embedding_matrix_google = load_word2vec_embeddings(embeddings_matrix_init,\\\n",
    "                                                   embeddings_index_google,\\\n",
    "                                                   tokenizer,VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_matrix_google' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a5cabbd84ea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/init_embedding_matrix_google_loaded'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_matrix_google\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding_matrix_google' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "np.save('./data/init_embedding_matrix_google_loaded',embedding_matrix_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_google = np.load('./data/init_embedding_matrix_google.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du réseau de Yoon Kim pour word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FILTERS = 100\n",
    "MODEL_NAME = \"embed_LSTM_BIDIR_word2vec_emb300_pretrained_trainableTrue_goodStatInit\"\n",
    "\n",
    "model_google = Bidirectional_LSTM(SENTENCE_LENGTH,VOCAB_SIZE,EMBEDDING_DIM_GOOGLE,\\\n",
    "                embedding_matrix_google,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 2095s 15ms/step - loss: 0.0596 - acc: 0.9798 - val_loss: 0.0454 - val_acc: 0.9838\n",
      "epoch: 1 - val_roc_auc: 0.9799\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 2098s 15ms/step - loss: 0.0414 - acc: 0.9841 - val_loss: 0.0429 - val_acc: 0.9839\n",
      "epoch: 2 - val_roc_auc: 0.9853\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 2\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_valid, y_valid))\n",
    "\n",
    "hist = model_google.fit(X_train, y_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=N_EPOCHS, \n",
    "                 validation_data=(X_valid, y_valid),\n",
    "                 callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained nnet to disk for later use\n",
    "save_nnet(model_google, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score on train set : 0.9927\n",
      "ROC-AUC score on validation set : 0.9853\n"
     ]
    }
   ],
   "source": [
    "# final model evaluation\n",
    "y_train_pred = model_google.predict(X_train, batch_size=512)\n",
    "train_score = evaluate(y_train, y_train_pred)\n",
    "print(\"ROC-AUC score on train set : {:.4f}\".format(train_score)) \n",
    "\n",
    "y_valid_pred = model_google.predict(X_valid, batch_size=512)\n",
    "valid_score = evaluate(y_valid, y_valid_pred)\n",
    "print(\"ROC-AUC score on validation set : {:.4f}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_test_pred = model_google.predict(X_test, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write submission file\n",
    "submission(y_test_pred, id_test, name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. What about Freebase ?\n",
    "\n",
    "https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Use pre-trained GloVe words for embeddings with LSTM model\n",
    "\n",
    "## embedding_matrix avec les embeddings de Glove\n",
    "\n",
    "copy of code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word of embeddings vector below is: -0.29736\n",
      "(199,)\n",
      "Loaded 1193513 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe pre-trained embeddings\n",
    "EMBEDDING_DIM = 200  # several embeddings sizes possible with Glove : 50, 100, 200, 300 \n",
    "GLOVE_EMBEDDINGS = 'glove.twitter.27B/glove.twitter.27B.{}d.txt'.format(EMBEDDING_DIM)\n",
    "\n",
    "embeddings_index = dict()\n",
    "with open(GLOVE_EMBEDDINGS) as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        if coefs.shape != (200,):\n",
    "            print(\"word of embeddings vector below is:\",word)\n",
    "            print(coefs.shape)\n",
    "            continue\n",
    "        #print(coefs.shape) # le code est bien compatible avec le format renvoyé par word2vec\n",
    "        embeddings_index[word] = coefs\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a weight matrix for words in training docs\n",
    "\n",
    "# get mean and std values of pre-trained embeddings\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "# print(all_embs.shape)\n",
    "emb_mean, emb_std = np.mean(all_embs, axis=0), np.std(all_embs, axis=0)\n",
    "# print(emb_mean)\n",
    "# print(emb_std) # les prints sont là pour évaluer comment faire l'initialisation arbitraire pour word2vec\n",
    "\n",
    "# init matrix to embeddings distribution\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (VOCAB_SIZE, EMBEDDING_DIM))\n",
    "# print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with a GloVe embedding: 26798\n",
      "Percentage of words with a GloVe embedding: 0.8932666666666667\n"
     ]
    }
   ],
   "source": [
    "# loop on words in our documents\n",
    "\n",
    "wordWithGloveEmb = 0\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    # if word isn't enough used in documents, ignore it\n",
    "    if index >= VOCAB_SIZE: \n",
    "        continue\n",
    "    # otherwise, fill embedding matrix with pre-trained vector corresponding to this word\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        wordWithGloveEmb += 1\n",
    "        embedding_matrix[index, :] = embedding_vector\n",
    "        \n",
    "print(\"Number of words with a GloVe embedding:\",wordWithGloveEmb)\n",
    "print(\"Percentage of words with a GloVe embedding:\",wordWithGloveEmb/VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du réseau LSTM pour GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"draft_embed_bidirlstm_2fc_EMB_PRETRAINED_GLOVE200t_TWITTER\"\n",
    "\n",
    "# input\n",
    "inp = Input(shape=(SENTENCE_LENGTH, ))\n",
    "# embedding\n",
    "x = Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=SENTENCE_LENGTH, \n",
    "                weights=[embedding_matrix], trainable=True)(inp)\n",
    "# LSTM\n",
    "x = Bidirectional(LSTM(60, return_sequences=True,name='lstm_layer'))(x)\n",
    "# max pooling 1D\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "# dropout 1\n",
    "x = Dropout(0.1)(x)\n",
    "# dense 1\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "# dropout 2\n",
    "x = Dropout(0.1)(x)\n",
    "# dense 1\n",
    "outp = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "# # load pre-trained model from disk\n",
    "# model = load_nnet(MODEL_NAME)\n",
    "\n",
    "model = Model(inputs=inp, outputs=outp)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "  5824/143613 [>.............................] - ETA: 28:16 - loss: 0.1313 - acc: 0.9605"
     ]
    }
   ],
   "source": [
    "# train\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 2\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_valid, y_valid))\n",
    "\n",
    "hist = model.fit(X_train, y_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=N_EPOCHS, \n",
    "                 validation_data=(X_valid, y_valid),\n",
    "                 callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained nnet to disk for later use\n",
    "save_nnet(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score on train set : 0.9933\n",
      "ROC-AUC score on validation set : 0.9867\n"
     ]
    }
   ],
   "source": [
    "# final model evaluation\n",
    "y_train_pred = model.predict(X_train, batch_size=512)\n",
    "train_score = evaluate(y_train, y_train_pred)\n",
    "print(\"ROC-AUC score on train set : {:.4f}\".format(train_score)) \n",
    "\n",
    "y_valid_pred = model.predict(X_valid, batch_size=512)\n",
    "valid_score = evaluate(y_valid, y_valid_pred)\n",
    "print(\"ROC-AUC score on validation set : {:.4f}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_test_pred = model.predict(X_test, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write submission file\n",
    "submission(y_test_pred, id_test, name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mars_env",
   "language": "python",
   "name": "mars_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
