{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan de ce notebook\n",
    "\n",
    "1. Use pre-trained GloVe words for embeddings\n",
    "2. Use pre-trained word2vec words for embeddings\n",
    "3. What about Freebase ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pour standardiser le nom des variables: GloVe _stanford et word2vec _google ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Use pre-trained GloVe words for embeddings\n",
    "\n",
    "https://medium.com/@sabber/classifying-yelp-review-comments-using-cnn-lstm-and-pre-trained-glove-word-embeddings-part-3-53fcea9a17fa\n",
    "\n",
    "https://github.com/msahamed/yelp_comments_classification_nlp\n",
    "\n",
    "https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "#from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GlobalMaxPooling1D, Bidirectional, Conv1D, concatenate\n",
    "from keras.layers import Dense, Input, LSTM, Dropout, Activation, GlobalMaxPooling1D, Bidirectional, Conv1D, concatenate\n",
    "\n",
    "# apparemment pour définir l'embeddings \"pré entraîné\" il\n",
    "# faut importer un Embedding différent (??)\n",
    "# pas trouvé dans le doc Keras - https://keras.io/layers/embeddings/\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw string data\n",
    "data_train, y_train_all, data_test, id_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des données (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
      "-------\n",
      "Hey man I m really not trying to edit war It s just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page He seems to care more about the formatting than the actual info \n"
     ]
    }
   ],
   "source": [
    "params = {'lower': False, \n",
    "          'lemma': False, \n",
    "          'stop_words': False}\n",
    "\n",
    "comment = data_train[2]\n",
    "print(comment)\n",
    "print('-------')\n",
    "print(clean_comment(comment, **params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation: 100%       \n",
      "Transformation: 100%       \n"
     ]
    }
   ],
   "source": [
    "clean_data_train = transform_dataset(data_train, func=clean_comment, kwargs=params)\n",
    "clean_data_test = transform_dataset(data_test, func=clean_comment, kwargs=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition des jeux de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert strings to int indexes, \n",
    "# considering only the VOCAB_SIZE most commons words, \n",
    "# and pad the sentences to SENTENCE_LENGTH words\n",
    "VOCAB_SIZE = 30000\n",
    "SENTENCE_LENGTH = 200  # 200 if stop_words deleted, 120 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODING: Fitting vectorizer to data\n",
      "ENCODING: transforming data to numerical\n"
     ]
    }
   ],
   "source": [
    "tokens_vectorizer = TokenVectorizer(max_len=SENTENCE_LENGTH, max_features=VOCAB_SIZE)\n",
    "\n",
    "# X_train_all, X_test = encode(data_train, data_test, vectorizer=tokens_vectorizer)\n",
    "X_train_all, X_test = encode(clean_data_train, clean_data_test, vectorizer=tokens_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_VALID_RATIO = 0.10\n",
    "SPLIT_RANDOM_SEED = 0  # TODO : check split because of unbalanced classes\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all, \n",
    "                                                      test_size=SPLIT_VALID_RATIO,\n",
    "                                                      random_state=SPLIT_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization et découpage des données textuelles \n",
    "\n",
    "Conforme au github https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb\n",
    "\n",
    "pour pouvoir encode avec Glove après (ne pas encode directement != Nicolas sur models_testing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 200)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words= VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(data_train)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(data_train)\n",
    "data = pad_sequences(sequences,maxlen=SENTENCE_LENGTH)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_VALID_RATIO = 0.10\n",
    "SPLIT_RANDOM_SEED = 0  # TODO : check split because of unbalanced classes\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(data, y_train_all, \n",
    "                                                      test_size=SPLIT_VALID_RATIO,\n",
    "                                                      random_state=SPLIT_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding_matrix avec les embeddings de Glove\n",
    "\n",
    "several possibilities of pre-training/embeddings vector sizes for GloVe, see:\n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "dans Mikolov c'est du 300 pour la taille des embeddings https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    #print(coefs.shape) # le code est bien compatible avec le format renvoyé par word2vec\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# several embeddings sizes possible with Glove\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > VOCAB_SIZE - 1: # détermine à quel point on s'intéresse aux mots moins importants d'après Glove\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word) # on va chercher le mot dans Glove embeddings_index\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du réseau de Yoon Kim pour GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FILTERS = 100\n",
    "MODEL_NAME = \"embed_conv_fc_GLOVE\"\n",
    "\n",
    "# input\n",
    "inp = Input(shape=(SENTENCE_LENGTH, ))\n",
    "# embedding\n",
    "emb = Embedding(VOCAB_SIZE, EMBEDDING_DIM,input_length=SENTENCE_LENGTH,weights=[embedding_matrix], trainable=False)(inp)\n",
    "\n",
    "# Specify each convolution layer and their kernel siz i.e. n-grams \n",
    "conv_3 = Conv1D(filters=N_FILTERS, kernel_size=3, activation='relu')(emb)\n",
    "pool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "\n",
    "conv_4 = Conv1D(filters=N_FILTERS, kernel_size=4, activation='relu')(emb)\n",
    "pool_4 = GlobalMaxPooling1D()(conv_4)\n",
    "\n",
    "conv_5 = Conv1D(filters=N_FILTERS, kernel_size=5, activation='relu')(emb)\n",
    "pool_5 = GlobalMaxPooling1D()(conv_5)\n",
    "\n",
    "# Gather all convolution layers\n",
    "x = concatenate([pool_3, pool_4, pool_5], axis=1)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outp = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "# # load pre-trained model from disk\n",
    "# model = load_nnet(MODEL_NAME)\n",
    "\n",
    "model = Model(inputs=inp, outputs=outp)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/4\n",
      "143613/143613 [==============================] - 528s 4ms/step - loss: 0.2888 - acc: 0.9634 - val_loss: 0.1604 - val_acc: 0.9629\n",
      "epoch: 1 - val_roc_auc: 0.5000\n",
      "Epoch 2/4\n",
      "143613/143613 [==============================] - 530s 4ms/step - loss: 0.1465 - acc: 0.9634 - val_loss: 0.1431 - val_acc: 0.9629\n",
      "epoch: 2 - val_roc_auc: 0.5000\n",
      "Epoch 3/4\n",
      "143613/143613 [==============================] - 530s 4ms/step - loss: 0.1411 - acc: 0.9634 - val_loss: 0.1424 - val_acc: 0.9629\n",
      "epoch: 3 - val_roc_auc: 0.5000\n",
      "Epoch 4/4\n",
      "143613/143613 [==============================] - 532s 4ms/step - loss: 0.1409 - acc: 0.9634 - val_loss: 0.1424 - val_acc: 0.9629\n",
      "epoch: 4 - val_roc_auc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 2\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_valid, y_valid))\n",
    "\n",
    "hist = model.fit(X_train, y_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=N_EPOCHS, \n",
    "                 validation_data=(X_valid, y_valid),\n",
    "                 callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained nnet to disk for later use\n",
    "save_nnet(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score on train set : 0.5000\n",
      "ROC-AUC score on validation set : 0.5000\n"
     ]
    }
   ],
   "source": [
    "# final model evaluation\n",
    "y_train_pred = model.predict(X_train, batch_size=512)\n",
    "train_score = evaluate(y_train, y_train_pred)\n",
    "print(\"ROC-AUC score on train set : {:.4f}\".format(train_score)) \n",
    "\n",
    "y_valid_pred = model.predict(X_valid, batch_size=512)\n",
    "valid_score = evaluate(y_valid, y_valid_pred)\n",
    "print(\"ROC-AUC score on validation set : {:.4f}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_test_pred = model.predict(X_test, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write submission file\n",
    "submission(y_test_pred, id_test, name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Use pre-trained word2vec words for embeddings\n",
    "\n",
    "le github précédent utilise word2vec pour entraîner sur le corpus du problème même, et pas comme source d'embeddings pré entraînés... https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb\n",
    "\n",
    "\"In this subsection, I use word2vec to create word embeddings from the review comments. Word2vec is one algorithm for learning a word embedding from a text corpus.\" --->>> à la base word2vec c'est le réseau d'extraction, on veut récupérer un résultat d'entraînement de référence de cette architecture !\n",
    "\n",
    "Il faut utiliser word2vec entraîné sur Google News, embeddings de taille 300, par Mikolov https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "Post de blog chargeant word2vec pré-entraîné par Mikolov:\n",
    "\n",
    "https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "filename = 'GoogleNews-vectors-negative300.bin/data'\n",
    "google_model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in pre trained word2vec: 3000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbauw/Documents/Académique/M2_AIC/OPT1_DataCamp/mars_craters/mars_env/lib/python3.5/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "google_model_words = list(google_model.wv.vocab)\n",
    "print(\"Number of words in pre trained word2vec:\",len(google_model_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(google_model['fuck']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model['fuck'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding_matrix avec les embeddings de google news word2vec\n",
    "\n",
    "on s'inspire de la construction de la matrice pour GloVe, \n",
    "ici pas besoin de construire le dictionnaire à partir du modèle\n",
    "\n",
    "notons que le tokenizer est déjà initialisé et reste le même"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes inchangées:\n",
    "# VOCAB_SIZE = 30000\n",
    "# SENTENCE_LENGTH = 200  # 200 if stop_words deleted, 120 otherwise\n",
    "\n",
    "# ONLY ONE embedding size possible with google news trained word2vec\n",
    "EMBEDDING_DIM_GOOGLE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix_google = np.zeros((VOCAB_SIZE, EMBEDDING_DIM_GOOGLE))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > VOCAB_SIZE - 1: # détermine à quel point on s'intéresse aux mots moins importants d'après Glove\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector_google = google_model[word] # on va chercher le mot dans word2vec embeddings\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix_google[index] = embedding_vector_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_google.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du réseau de Yoon Kim pour word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FILTERS = 100\n",
    "MODEL_NAME_GOOGLE = \"embed_conv_fc_GOOGLE\"\n",
    "\n",
    "# input\n",
    "inp = Input(shape=(SENTENCE_LENGTH, ))\n",
    "# embedding\n",
    "emb = Embedding(VOCAB_SIZE, EMBEDDING_DIM_GOOGLE,input_length=SENTENCE_LENGTH,weights=[embedding_matrix_google], trainable=False)(inp)\n",
    "\n",
    "# Specify each convolution layer and their kernel siz i.e. n-grams \n",
    "conv_3 = Conv1D(filters=N_FILTERS, kernel_size=3, activation='relu')(emb)\n",
    "pool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "\n",
    "conv_4 = Conv1D(filters=N_FILTERS, kernel_size=4, activation='relu')(emb)\n",
    "pool_4 = GlobalMaxPooling1D()(conv_4)\n",
    "\n",
    "conv_5 = Conv1D(filters=N_FILTERS, kernel_size=5, activation='relu')(emb)\n",
    "pool_5 = GlobalMaxPooling1D()(conv_5)\n",
    "\n",
    "# Gather all convolution layers\n",
    "x = concatenate([pool_3, pool_4, pool_5], axis=1)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outp = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "# # load pre-trained model from disk\n",
    "# model = load_nnet(MODEL_NAME)\n",
    "\n",
    "model_google = Model(inputs=inp, outputs=outp)\n",
    "model_google.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1552s 11ms/step - loss: 0.2890 - acc: 0.9634 - val_loss: 0.1603 - val_acc: 0.9629\n",
      "epoch: 1 - val_roc_auc: 0.5000\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1083s 8ms/step - loss: 0.1464 - acc: 0.9634 - val_loss: 0.1431 - val_acc: 0.9629\n",
      "epoch: 2 - val_roc_auc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 2\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_valid, y_valid))\n",
    "\n",
    "hist = model_google.fit(X_train, y_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=N_EPOCHS, \n",
    "                 validation_data=(X_valid, y_valid),\n",
    "                 callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained nnet to disk for later use\n",
    "save_nnet(model_google, MODEL_NAME_GOOGLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score on train set : 0.5000\n",
      "ROC-AUC score on validation set : 0.5000\n"
     ]
    }
   ],
   "source": [
    "# final model evaluation\n",
    "y_train_pred = model_google.predict(X_train, batch_size=512)\n",
    "train_score = evaluate(y_train, y_train_pred)\n",
    "print(\"ROC-AUC score on train set : {:.4f}\".format(train_score)) \n",
    "\n",
    "y_valid_pred = model_google.predict(X_valid, batch_size=512)\n",
    "valid_score = evaluate(y_valid, y_valid_pred)\n",
    "print(\"ROC-AUC score on validation set : {:.4f}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_test_pred = model_google.predict(X_test, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write submission file\n",
    "submission(y_test_pred, id_test, name=MODEL_NAME_GOOGLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. What about Freebase ?\n",
    "\n",
    "https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mars_env",
   "language": "python",
   "name": "mars_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
